{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Object Detection.ipynb","provenance":[{"file_id":"1PNJ3lQv7-A_I9fvjKlZm3aRRkEloYJzT","timestamp":1622638242238},{"file_id":"1FzK4xjSmDFLETARVtzdC69lmfKZhyHIP","timestamp":1622604802120}],"collapsed_sections":["Q9hhlhDDMEos"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CtLPLbA0cxBE"},"source":["## Downloading Dependencies"]},{"cell_type":"code","metadata":{"id":"2h7Cxnb-5XHg"},"source":["# First, we need to install pycocotools. \n","# This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.\n","# The version by default in Colab has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n","!pip install cython\n","!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MbAivUSg_POH"},"source":["We have a number of helper functions to simplify training and evaluating detection models.\n","\n","We will import these dependencies `references/detection/utils.py` and `references/detection/coco_eval.py`."]},{"cell_type":"code","metadata":{"id":"GCvVCyiR_Lq-"},"source":["%%shell\n","\n","# Download TorchVision repo to use some files from references/detection\n","git clone https://github.com/pytorch/vision.git\n","cd vision\n","git checkout v0.3.0\n","\n","cp references/detection/utils.py ../\n","cp references/detection/coco_eval.py ../"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xealk6oLy1nH"},"source":["import json\n","import math\n","import os\n","import random\n","import sys\n","import time\n","import utils\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","import numpy as np\n","import torch\n","import torch.utils.data\n","import torchvision\n","import cv2 as cv\n","import torchvision.models as models\n","\n","from PIL import Image, ImageDraw\n","from typing import List, Tuple\n","from coco_eval import CocoEvaluator\n","from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval\n","from torchvision import transforms\n","from torchvision.ops import batched_nms\n","from torchvision.transforms import functional as F\n","from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n","from torchvision.models.detection.faster_rcnn import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.ops import MultiScaleRoIAlign"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUiwwAIP6_o_"},"source":["# check if cuda GPU is available, make sure you're using GPU runtime on Google Colab\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device) # you should output \"cuda\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XVTPR2eyuqDO"},"source":["## Importing Dataset from Google Drive"]},{"cell_type":"markdown","metadata":{"id":"0LH5qLV0vfiD"},"source":["The structure & format of dataset is defined under `Object Detection Database` below, in the next section."]},{"cell_type":"code","metadata":{"id":"JNnVy01g2FRO"},"source":["# mount your google drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","base_folder = '/content/drive/MyDrive' # Change this to your desired path\n","\n","# You should have uploaded the datasets onto your google drive.\n","training_path = os.path.join(base_folder, \"training_dataset.zip\") # Change this path to the zip file for the training images\n","!unzip $training_path\n","\n","testing_path = os.path.join(base_folder, \"test_dataset.zip\") # Change this path to the zip file for the test images\n","!unzip $testing_path"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t9z4qZ8A8HTn"},"source":["## Object Detection Dataset"]},{"cell_type":"markdown","metadata":{"id":"8KW4FNBBCt4E"},"source":["The data is structured as follows\n","```\n","c1_release/\n","  images/\n","    0001e6adc4fbab0c.jpg\n","    00067fe83e3e21c8.jpg\n","    0008ab3d8674f6ca.jpg\n","    ...\n","  labels.json\n","  train.json\n","  val.json\n","```\n","\n","`labels.json` contains the labels for the whole dataset (`train.json` + `val.json`) if you need it.\n","\n","The [torchvision reference scripts for training object detection](https://github.com/pytorch/vision/tree/v0.3.0/references/detection) allows for easily supporting adding new custom datasets.\n","The dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n","\n","The only specificity that we require is that the dataset `__getitem__` should return:\n","\n","* image: a PIL Image of size (H, W)\n","* target: a dict containing the following fields\n","    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n","    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n","    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n","    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n","    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n","\n","If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.\n","\n","Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.\n","\n","Let's write a `torch.utils.data.Dataset` class for this dataset."]},{"cell_type":"code","metadata":{"id":"j8Iv0DXlINyH"},"source":["class TILDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, annotation, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        self.coco = COCO(annotation)\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","        cats = self.coco.loadCats(self.coco.getCatIds())\n","        self.cat2name = {cat['id']:cat['name'] for cat in cats} # maps category id to category name (useful for visualization)\n","\n","    def __getitem__(self, index):\n","        coco = self.coco\n","        img_id = self.ids[index] # Image ID\n","        ann_ids = coco.getAnnIds(imgIds=img_id) # get annotation id from coco\n","        coco_annotation = coco.loadAnns(ann_ids) # target coco_annotation file for an image\n","        path = coco.loadImgs(img_id)[0]['file_name'] # path for input image\n","        img = Image.open(os.path.join(self.root, 'images', path)).convert('RGB') # open the input image\n","\n","        # number of objects in the image\n","        num_objs = len(coco_annotation)\n","\n","        # Bounding boxes for objects\n","        # In coco format, bbox = [xmin, ymin, width, height]\n","        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n","        boxes = []\n","        for i in range(num_objs):\n","            xmin = coco_annotation[i]['bbox'][0]\n","            ymin = coco_annotation[i]['bbox'][1]\n","            xmax = xmin + coco_annotation[i]['bbox'][2]\n","            ymax = ymin + coco_annotation[i]['bbox'][3]\n","            boxes.append([xmin, ymin, xmax, ymax])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","\n","        # Labels\n","        labels = []\n","        for i in range(num_objs):\n","            labels.append(coco_annotation[i]['category_id'])\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","\n","        # Tensorise img_id\n","        img_id = torch.tensor([img_id])\n","\n","        # Size of bbox (Rectangular)\n","        areas = []\n","        for i in range(num_objs):\n","            areas.append(coco_annotation[i]['area'])\n","        areas = torch.as_tensor(areas, dtype=torch.float32)\n","\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        # Annotation is in dictionary format\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"image_id\"] = img_id\n","        target[\"area\"] = areas\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.ids)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWm6AvzjDdLs"},"source":["That's all for the dataset. Let's see how the outputs are structured for this dataset"]},{"cell_type":"code","metadata":{"id":"19AIZfTM4R4a"},"source":["dataset_root = \"/content/training_dataset\" # extracted training dataset path\n","train_annotation = os.path.join(dataset_root, \"train.json\")\n","val_annotation = os.path.join(dataset_root, \"val.json\")\n","\n","dataset = TILDataset(dataset_root, train_annotation)\n","dataset[30]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jNUhbVfc4xVz"},"source":["So we can see that by default, the dataset returns a `PIL.Image` and a dictionary containing several fields, including `boxes` and `labels`.\n","\n","Let's next look at one example from the training set."]},{"cell_type":"code","metadata":{"id":"vgvaYqPf6uyo"},"source":["source_img, img_annots = dataset[30]\n","draw = ImageDraw.Draw(source_img)\n","for i in range(len(img_annots[\"boxes\"])):\n","    x1, y1, x2, y2 = img_annots[\"boxes\"][i]\n","    label = int(img_annots[\"labels\"][i])\n","\n","    draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n","    text = f'{dataset.cat2name[label]}'\n","    draw.text((x1+5, y1+5), text)\n","display(source_img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3OWoxpWcM3D4"},"source":["## Setting up the Model"]},{"cell_type":"markdown","metadata":{"id":"oJRdz9In3pAE"},"source":["I will be using a Faster R-CNN model with a ResNet50-FPN backbone."]},{"cell_type":"code","metadata":{"id":"c96yh1yaykGE"},"source":["# hyper-parameters\n","params = {'BATCH_SIZE': 16,\n","          'LR': 0.003,\n","          'CLASSES': 6,\n","          'MAXEPOCHS': 30,\n","          'BACKBONE': 'resnet50',\n","          'FPN': True,\n","          'ANCHOR_SIZE': ((32,), (64,), (128,), (256,), (512,)),\n","          'ASPECT_RATIOS': ((0.5, 1.0, 2.0),),\n","          'MIN_SIZE': 512,\n","          'MAX_SIZE': 512,\n","          'IMG_MEAN': [0.485, 0.456, 0.406],\n","          'IMG_STD': [0.229, 0.224, 0.225],\n","          'IOU_THRESHOLD': 0.5\n","          }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a0GWSdRqy9pA"},"source":["def get_resnet_backbone(backbone_name: str):\n","  \n","    if backbone_name == 'resnet18':\n","        pretrained_model = models.resnet18(pretrained=True, progress=False)\n","        out_channels = 512\n","    elif backbone_name == 'resnet34':\n","        pretrained_model = models.resnet34(pretrained=True, progress=False)\n","        out_channels = 512\n","    elif backbone_name == 'resnet50':\n","        pretrained_model = models.resnet50(pretrained=True, progress=False)\n","        out_channels = 2048\n","    elif backbone_name == 'resnet101':\n","        pretrained_model = models.resnet101(pretrained=True, progress=False)\n","        out_channels = 2048\n","    elif backbone_name == 'resnet152':\n","        pretrained_model = models.resnet152(pretrained=True, progress=False)\n","        out_channels = 2048\n","\n","    backbone = torch.nn.Sequential(*list(pretrained_model.children())[:-2])\n","    backbone.out_channels = out_channels\n","\n","    return backbone"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Un4UIW_d8PEe"},"source":["def get_resnet_fpn_backbone(backbone_name: str):\n","  \n","    return resnet_fpn_backbone(backbone_name, pretrained=True, trainable_layers=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W5oH8VNty-kX"},"source":["def get_anchor_generator(anchor_size: Tuple[tuple] = None, aspect_ratios: Tuple[tuple] = None):\n","\n","    if anchor_size is None:\n","        anchor_size = ((16,), (32,), (64,), (128,))\n","    if aspect_ratios is None:\n","        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_size)\n","\n","    anchor_generator = AnchorGenerator(sizes=anchor_size, aspect_ratios=aspect_ratios)\n","    return anchor_generator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uj92yxjOzNft"},"source":["def get_roi_pool(featmap_names: List[str] = None, output_size: int = 7, sampling_ratio: int = 2):\n","\n","    if featmap_names is None:\n","        # default for resnet with FPN\n","        featmap_names = ['0', '1', '2', '3']\n","\n","    roi_pooler = MultiScaleRoIAlign(featmap_names=featmap_names,\n","                                    output_size=output_size,\n","                                    sampling_ratio=sampling_ratio)\n","\n","    return roi_pooler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQNv6A4zzRqc"},"source":["def get_fasterRCNN(backbone: torch.nn.Module,\n","                   anchor_generator: AnchorGenerator,\n","                   roi_pooler: MultiScaleRoIAlign,\n","                   num_classes: int,\n","                   image_mean: List[float] = [0.485, 0.456, 0.406],\n","                   image_std: List[float] = [0.229, 0.224, 0.225],\n","                   min_size: int = 512,\n","                   max_size: int = 1024,\n","                   **kwargs\n","                   ):\n","\n","    model = FasterRCNN(backbone=backbone,\n","                       rpn_anchor_generator=anchor_generator,\n","                       box_roi_pool=roi_pooler,\n","                       num_classes=num_classes,\n","                       image_mean=image_mean,  # ImageNet\n","                       image_std=image_std,  # ImageNet\n","                       min_size=min_size,\n","                       max_size=max_size,\n","                       **kwargs\n","                       )\n","    model.num_classes = num_classes\n","    model.image_mean = image_mean\n","    model.image_std = image_std\n","    model.min_size = min_size\n","    model.max_size = max_size\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wfn_4EbLzfHf"},"source":["def get_fasterRCNN_resnet(num_classes: int,\n","                          backbone_name: str,\n","                          anchor_size: List[float],\n","                          aspect_ratios: List[float],\n","                          fpn: bool = True,\n","                          min_size: int = 512,\n","                          max_size: int = 1024,\n","                          **kwargs\n","                          ):\n","\n","    # Backbone\n","    if fpn:\n","        backbone = get_resnet_fpn_backbone(backbone_name=backbone_name)\n","    else:\n","        backbone = get_resnet_backbone(backbone_name=backbone_name)\n","\n","    # Anchors\n","    anchor_size = anchor_size\n","    aspect_ratios = aspect_ratios * len(anchor_size)\n","    anchor_generator = get_anchor_generator(anchor_size=anchor_size, aspect_ratios=aspect_ratios)\n","\n","    # ROI Pool\n","    with torch.no_grad():\n","        backbone.eval()\n","        random_input = torch.rand(size=(1, 3, 512, 512))\n","        features = backbone(random_input)\n","\n","    if isinstance(features, torch.Tensor):\n","        from collections import OrderedDict\n","\n","        features = OrderedDict([('0', features)])\n","\n","    featmap_names = [key for key in features.keys() if key.isnumeric()]\n","\n","    roi_pool = get_roi_pool(featmap_names=featmap_names)\n","\n","    # Model\n","    return get_fasterRCNN(backbone=backbone,\n","                          anchor_generator=anchor_generator,\n","                          roi_pooler=roi_pool,\n","                          num_classes=num_classes,\n","                          min_size=min_size,\n","                          max_size=max_size,\n","                          **kwargs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hljmqSJOzibW"},"source":["model = get_fasterRCNN_resnet(num_classes=params['CLASSES'],\n","                              backbone_name=params['BACKBONE'],\n","                              anchor_size=params['ANCHOR_SIZE'],\n","                              aspect_ratios=params['ASPECT_RATIOS'],\n","                              fpn=params['FPN'],\n","                              min_size=params['MIN_SIZE'],\n","                              max_size=params['MAX_SIZE'],\n","                              image_mean=params['IMG_MEAN'],\n","                              image_std=params['IMG_STD'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FpG3AhNjOJ6-"},"source":["# load pretrained weights for FasterRCNN ResNet50 FPN\n","pretrained_dict = torch.hub.load_state_dict_from_url('https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth', progress=True)\n","model_dict = model.state_dict()\n","\n","# filter out roi_heads.box_predictor weights\n","pretrained_dict = {k: v for k, v in pretrained_dict.items() if not k.startswith('roi_heads.box_predictor')}\n","# overwrite entries in the existing state dict\n","model_dict.update(pretrained_dict)\n","# load the new state dict\n","model.load_state_dict(model_dict)\n","# moving model to the device\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5iPlWCjV28P5"},"source":["# construct an optimizer\n","model_params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(model_params, lr=params['LR'], momentum=0.9, weight_decay=0.0005)\n","\n","# and a learning rate scheduler which decreases the learning rate by 10x every 3 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E3_rz0d4Khpn"},"source":["## Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"p5rN6Q623cKV"},"source":["Add more augmentations to make the model more robust to noise and other unseen circumstances"]},{"cell_type":"markdown","metadata":{"id":"auand24_xRrd"},"source":["### Wrapper to apply all transformations"]},{"cell_type":"code","metadata":{"id":"Bmr8kmOZhJ91"},"source":["class Compose(object):\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, image, target):\n","        for t in self.transforms:\n","            image, target = t(image, target)\n","        return image, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xGB3yBLP0dKP"},"source":["### Applies a random box zoom to the image with a given probability"]},{"cell_type":"code","metadata":{"id":"JJc1IJ5YyMHd"},"source":["class RandomBoxZoom(object):\n","  def __init__(self, prob):\n","    self.prob = prob\n","\n","  def __call__(self, image, target):\n","    if random.random() < self.prob:\n","      # index = round(random.uniform(0,len(target['boxes']),len(target['boxes'])))\n","      index = int(random.random() * len(target['boxes']))\n","      label = target[\"labels\"].numpy()\n","      boxes = target[\"boxes\"].numpy()\n","      area = target[\"area\"].numpy()\n","      iscrowd = target['iscrowd'].numpy()\n","      for i in range(len(target[\"boxes\"])):\n","        if i == index:\n","          x1, y1, x2, y2 = boxes[i]\n","          boxes[i] = [x1-x1,y1-y1,x2-x1,y2-y1]\n","          image = image.crop((x1,y1,x2,y2))\n","          target['labels'] = torch.tensor(np.expand_dims(label[i], axis=0))\n","          target['boxes'] = torch.tensor(np.expand_dims(boxes[i], axis=0))\n","          target['area'] = torch.tensor(np.expand_dims(area[i], axis=0))\n","          target['iscrowd'] = torch.tensor(np.expand_dims(iscrowd[i], axis=0))\n","    return image, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NOi6pYgS-gn1"},"source":["### Decommisioned Augmentations"]},{"cell_type":"markdown","metadata":{"id":"QbxfBvM8zy8B"},"source":["#### Converts a PIL image into a Numpy"]},{"cell_type":"code","metadata":{"id":"Qwi8xLVFnDdr"},"source":["class imgToNumpy(object):\n","    def __call__(self, image, target):\n","        image = np.array(image)\n","        image = image[:, :, ::-1].copy()\n","        return image, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dn-nMMgX0CGi"},"source":["#### Applies a random zoom with a given probability"]},{"cell_type":"code","metadata":{"id":"eU1XuisSwpkj"},"source":["class RandomZoom(object):\n","  def __init__(self, prob):\n","    self.prob = prob\n","\n","  def __call__(self, image, target):\n","    if random.random() < self.prob:\n","      value = random.random()*0.5 + 0.5\n","      height, width = image.shape[:2]\n","      h_taken = int(value*height)\n","      w_taken = int(value*width)\n","      h_start = random.randint(0, height-h_taken)\n","      w_start = random.randint(0, width-w_taken)\n","      image = image[w_start:w_start+w_taken, h_start:h_start+h_taken, :]\n","\n","      bbox = target[\"boxes\"]\n","      bbox[:,[0,2]] = bbox[:,[0,2]] - w_start\n","      bbox[:,[1,3]] = bbox[:,[1,3]] - h_start\n","      \n","      label = target[\"labels\"].numpy()\n","      boxArr = bbox.numpy() \n","      for index,box in enumerate(boxArr):\n","          #Box lower limit > Height\n","          if box[0] > w_start + w_taken:\n","              np.delete(boxArr, index)\n","              np.delete(label, index)\n","              continue\n","          elif box[0] <= w_start + w_taken and box[0] >= w_start:\n","              if box[2] > w_start + w_taken:\n","                  box[2] = w_start + w_taken\n","          else:\n","              if box[2] < w_start:\n","                  np.delete(boxArr, index)\n","                  np.delete(label, index)\n","                  continue\n","              elif box[2] > w_start and box[2] <= w_start + w_taken:\n","                  box[0] = w_start\n","              else:\n","                  box[0] = w_start\n","                  box[2] = w_start + w_taken\n","\n","          if box[1] > h_start + h_taken:\n","              np.delete(boxArr, index)\n","              np.delete(label, index)\n","          elif box[1] <= h_start + h_taken and box[1] >= h_start:\n","              if box[3] > h_start + h_taken:\n","                  box[3] = h_start + h_taken\n","          else:\n","              if box[3] < h_start:\n","                  np.delete(boxArr, index)\n","                  np.delete(label, index)\n","              elif box[3] > h_start and box[3] <= h_start + h_taken:\n","                  box[1] = h_start\n","              else:\n","                  box[1] = h_start\n","                  box[3] = h_start + h_taken\n","    return image, target \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AiiICxjS0I49"},"source":["#### Applies a random channel shift to the image with a given probability"]},{"cell_type":"code","metadata":{"id":"rCzDww5ig12Q"},"source":["class RandomChannelShift(object):\n","  def __init__(self, prob, value):\n","      self.prob = prob\n","      self.value = value\n","  \n","  def __call__(self, image, target):\n","    if random.random() < self.prob:\n","      value = int(random.uniform(-self.value, self.value))\n","      image = image + value\n","      image[:,:,:][image[:,:,:]>255]  = 255\n","      image[:,:,:][image[:,:,:]<0]  = 0\n","      image = image.astype(np.uint8)\n","    return image, target\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YHYyPB7c0OV_"},"source":["#### Adjusts the brightness of the image randomly with a given probability"]},{"cell_type":"code","metadata":{"id":"9V6TVHDuhkHE"},"source":["class RandomBrightnessAdjustment(object):\n","  def __init__(self, prob, low, high):\n","    self.prob = prob\n","    self.low = low\n","    self.high = high\n","\n","  def __call__(self, image, target):\n","    if random.random() < self.prob:\n","      value = random.uniform(self.low, self.high)\n","      hsv = cv.cvtColor(image, cv.COLOR_BGR2HSV)\n","      hsv = np.array(hsv, dtype=np.float64)\n","      hsv[:,:,1] = hsv[:,:,1]*value\n","      hsv[:,:,1][hsv[:,:,1]>255]  = 255\n","      hsv[:,:,2] = hsv[:,:,2]*value \n","      hsv[:,:,2][hsv[:,:,2]>255]  = 255\n","      hsv = np.array(hsv, dtype = np.uint8)\n","      image = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n","    return image, target\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KfcO9lr8yIEz"},"source":["#### Blurs the image with a given probability"]},{"cell_type":"code","metadata":{"id":"blncMQyE14Yz"},"source":["class RandomBlur(object):\n","    def __init__(self, prob):\n","        self.prob = prob\n","\n","    def __call__(self, image, target):\n","        if random.random() < self.prob:\n","            blur = round(random.random() * 5)*2 + 5\n","            image = cv.GaussianBlur(image,(blur,blur),0)\n","        return image, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oNJ7xD-k0u_S"},"source":["#### Converts a Numpy back into a PIL Image"]},{"cell_type":"code","metadata":{"id":"BDDQ16Tynd7k"},"source":["class numpyToImg(object):\n","    def __call__(self, image, target):\n","        image = Image.fromarray(np.uint8(image)).convert('RGB')\n","        return image, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZ3EwPVQxt1U"},"source":["### Converts a PIL image into a PyTorch Tensor"]},{"cell_type":"code","metadata":{"id":"iNeXIW8shR0_"},"source":["class ToTensor(object):\n","    def __call__(self, image, target):\n","        image = F.to_tensor(image)\n","        return image, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G5WzYu9A01lE"},"source":["### Vertically flips the image and ground-truth labels with a given probability"]},{"cell_type":"code","metadata":{"id":"lmBbCT6U8pHp"},"source":["class RandomVerticalFlip(object):\n","  def __init__(self, prob):\n","    self.prob = prob\n","\n","  def __call__(self, image, target):\n","    if random.random() < self.prob:\n","      height, width = image.shape[-2:]\n","      image = image.flip(1)\n","      bbox = target[\"boxes\"]\n","      bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n","      target[\"boxes\"] = bbox\n","    return image, target\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uFTRHNL8x5J1"},"source":["### Horizontal flips the image and ground-truth labels with a given probability"]},{"cell_type":"code","metadata":{"id":"jwFX52gbhUQX"},"source":["class RandomHorizontalFlip(object):\n","    def __init__(self, prob):\n","        self.prob = prob\n","\n","    def __call__(self, image, target):\n","        if random.random() < self.prob:\n","            height, width = image.shape[-2:]\n","            image = image.flip(-1)\n","            bbox = target[\"boxes\"]\n","            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n","            target[\"boxes\"] = bbox\n","        return image, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p2fvUhBg06Um"},"source":["### A method which encapsulates all the augmentations"]},{"cell_type":"code","metadata":{"id":"omm4z5GsJ-wS"},"source":["def get_transform(train):\n","    if train:\n","        transforms = Compose([\n","            RandomBoxZoom(0.2),\n","\n","            # # The numpy augmentations below are computationally intensive, use at your own discretion\n","            # imgToNumpy(), \n","            # RandomZoom(0.2),\n","            # RandomChannelShift(1,60),\n","            # RandomBrightnessAdjustment(0.3,0.5,2),\n","            # RandomBlur(0.3),\n","            # numpyToImg(),\n","\n","            ToTensor(),\n","            RandomVerticalFlip(0.3),\n","            RandomHorizontalFlip(0.4)\n","        ])\n","    else: # during evaluation, no augmentations will be done\n","        transforms = Compose([\n","            ToTensor()\n","        ])\n","    \n","    return transforms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aqp_tYw1IXvI"},"source":["Normalisation and rescaling is not needed since there exists a layer that handles these augmentations in the R-CNN model. "]},{"cell_type":"markdown","metadata":{"id":"Q9hhlhDDMEos"},"source":["## Data Loaders"]},{"cell_type":"code","metadata":{"id":"0DBaaGV7JP1W"},"source":["NUM_WORKERS = 4\n","\n","# use our dataset and defined transformations\n","train_dataset = TILDataset(dataset_root, train_annotation, get_transform(train=True))\n","val_dataset = TILDataset(dataset_root, val_annotation, get_transform(train=False))\n","\n","# define training and validation data loaders\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=params['BATCH_SIZE'], shuffle=True, num_workers=NUM_WORKERS,\n","    collate_fn=utils.collate_fn)\n","\n","val_loader = torch.utils.data.DataLoader(\n","    val_dataset, batch_size=1, shuffle=False, num_workers=NUM_WORKERS,\n","    collate_fn=utils.collate_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0amZn1wMHvkP"},"source":["## Model Training"]},{"cell_type":"markdown","metadata":{"id":"hny8e1PO4Auw"},"source":["We will train the model and evaluate its performance at the end of every epoch."]},{"cell_type":"code","metadata":{"id":"QnNPceNj5vTc"},"source":["def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n","    model.train()\n","    metric_logger = utils.MetricLogger(delimiter=\"  \")\n","    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n","    header = 'Epoch: [{}]'.format(epoch)\n","\n","    lr_scheduler = None\n","    if epoch == 0:\n","        warmup_factor = 1. / 1000\n","        warmup_iters = min(1000, len(data_loader) - 1)\n","\n","        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n","\n","    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n","        #Addition\n","        \n","        #Addition\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","        loss_value = losses.item()\n","\n","        if not math.isfinite(loss_value):\n","            print(\"Loss is {}, stopping training\".format(loss_value))\n","            print(loss_dict)\n","            sys.exit(1)\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        if lr_scheduler is not None:\n","            lr_scheduler.step()\n","\n","        metric_logger.update(loss=losses, **loss_dict)\n","        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K8uKl51Z506h"},"source":["@torch.no_grad()\n","def evaluate(model, data_loader, device):\n","    n_threads = torch.get_num_threads()\n","    torch.set_num_threads(1)\n","    model.eval()\n","    metric_logger = utils.MetricLogger(delimiter=\"  \")\n","    header = 'Test:'\n","\n","    coco = data_loader.dataset.coco\n","    iou_types = [\"bbox\"]\n","    coco_evaluator = CocoEvaluator(coco, iou_types)\n","\n","    for image, targets in metric_logger.log_every(data_loader, 100, header):\n","        image = list(img.to(device) for img in image)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        torch.cuda.synchronize()\n","        model_time = time.time()\n","        outputs = model(image)\n","\n","        outputs = [{k: v for k, v in t.items()} for t in outputs]\n","        model_time = time.time() - model_time\n","\n","        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n","        evaluator_time = time.time()\n","        coco_evaluator.update(res)\n","        evaluator_time = time.time() - evaluator_time\n","        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n","\n","    # gather the stats from all processes\\\n","    metric_logger.synchronize_between_processes()\n","    print(\"Averaged stats:\", metric_logger)\n","    coco_evaluator.synchronize_between_processes()\n","\n","    # accumulate predictions from all images\n","    coco_evaluator.accumulate()\n","    coco_evaluator.summarize()\n","    torch.set_num_threads(n_threads)\n","    return coco_evaluator\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"trjdd2wgXqfv"},"source":["for epoch in range(params['MAXEPOCHS']):\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n","\n","    # update the learning rate\n","    lr_scheduler.step()\n","\n","    # evaluate on the test dataset\n","    evaluate(model, val_loader, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ACQjk3ekP7cc"},"source":["## Visualization of results"]},{"cell_type":"markdown","metadata":{"id":"n1vTbbN2Q1Ys"},"source":["FasterRCNN already has NMS built into it, which is used to remove duplicated predictions.\n","\n","We can further filter out the non-confident detections, using a threshold of 0.75."]},{"cell_type":"code","metadata":{"id":"HpfDgkUzRCSy"},"source":["det_threshold = 0.75\n","\n","# convert the image, which has been rescaled to 0-1 and had the channels flipped\n","img, _ = val_dataset[0]\n","pred_img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n","draw = ImageDraw.Draw(pred_img)\n","\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])\n","    \n","img_preds = prediction[0]\n","keep_idx = batched_nms(boxes=img_preds[\"boxes\"], scores=img_preds[\"scores\"], idxs=img_preds[\"labels\"], iou_threshold=params['IOU_THRESHOLD'])\n","\n","for i in range(len(img_preds[\"boxes\"])):\n","    if i in keep_idx:\n","        x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n","        label = int(img_preds[\"labels\"][i])\n","        score = float(img_preds[\"scores\"][i])\n","\n","        if score > det_threshold:\n","            draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n","            text = f'{dataset.cat2name[label]}: {score}'\n","            draw.text((x1+5, y1+5), text)\n","\n","display(pred_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lycezchIRPK7"},"source":["# save model weights\n","save_path = os.path.join(base_folder, \"object_detection_rcnn.pth\")\n","torch.save(model.state_dict(), save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivhZ0aMlg_nF"},"source":["## Evaluation on Validation Set"]},{"cell_type":"code","metadata":{"id":"uCbo_MyRhEVK"},"source":["det_threshold = 0.75\n","\n","with open(val_annotation) as json_file:\n","    val_data = json.load(json_file)\n","\n","model.eval()\n","detections = []\n","with torch.no_grad():\n","    for image in val_data[\"images\"]:\n","        img_name = image[\"file_name\"]\n","        img_id = image[\"id\"]\n","\n","        img = Image.open(os.path.join(dataset_root, 'images', img_name)).convert('RGB')\n","        img_tensor = transforms.ToTensor()(img)\n","\n","        preds = model([img_tensor.to(device)])[0]\n","\n","        for i in range(len(preds[\"boxes\"])):\n","            x1, y1, x2, y2 = preds[\"boxes\"][i]\n","            label = int(preds[\"labels\"][i])\n","            score = float(preds[\"scores\"][i])\n","\n","            left = int(x1)\n","            top = int(y1)\n","            width = int(x2 - x1)\n","            height = int(y2 - y1)\n","\n","            if score >= det_threshold:\n","                detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yymAERJJhcc_"},"source":["validation_json = os.path.join(base_folder, \"validation.json\")\n","with open(validation_json, 'w') as f:\n","    json.dump(detections, f)\n","\n","# Get evaluation score against validation set to make sure your prediction json file is in the correct format\n","coco_gt = COCO(val_annotation)\n","coco_dt = coco_gt.loadRes(validation_json)\n","cocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\n","cocoEval.evaluate()\n","cocoEval.accumulate()\n","cocoEval.summarize()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rUGlO1lFRQQd"},"source":["## Generate Predictions on Test Images"]},{"cell_type":"code","metadata":{"id":"VImfhYxKuvKz"},"source":["# load model weights (if not using the current trained model)\n","load_path = os.path.join(base_folder, \"object_detection_rcnn.pth\")\n","model.load_state_dict(torch.load(load_path, map_location=device))\n","model.to(device)\n","model.eval()\n","\n","dataset_test_root = \"/content/test_dataset\" # extracted testing images path\n","test_img_root = os.path.join(dataset_test_root, \"images\")\n","img_dir = os.scandir(test_img_root)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X1dODyYiTXSF"},"source":["Let's visualize some predictions on the test images. Run this a few times to visualize different images."]},{"cell_type":"code","metadata":{"id":"VBwlAvrCTZgU"},"source":["img = Image.open(next(img_dir).path).convert('RGB')\n","draw = ImageDraw.Draw(img)\n","det_threshold = 0.75\n","\n","# do the prediction\n","with torch.no_grad():\n","    img_tensor = transforms.ToTensor()(img)\n","    img_preds = model([img_tensor.to(device)])[0]\n","\n","for i in range(len(img_preds[\"boxes\"])):\n","    x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n","    label = int(img_preds[\"labels\"][i])\n","    score = float(img_preds[\"scores\"][i])\n","\n","    # filter out non-confident detections\n","    if score >= det_threshold:\n","        draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n","        text = f'{dataset.cat2name[label]}: {score}'\n","        draw.text((x1+5, y1+5), text)\n","\n","display(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qeGHrPzIa9_7"},"source":["**Output Results into a json file**\n","\n","Resultant json file will be in [COCO format](https://cocodataset.org/#format-results).\n","\n","```\n","[{\n","    \"image_id\": int, \n","    \"category_id\": int, \n","    \"bbox\": [x,y,width,height], \n","    \"score\": float,\n","}]\n","```"]},{"cell_type":"code","metadata":{"id":"aJqFPKWITzgo"},"source":["# generate detections on the folder of test images (this will be used for submission)\n","detections = []\n","det_threshold = 0.75\n","\n","with torch.no_grad():\n","    for image in img_dir:\n","        img_id = int(image.name.split('.')[0])\n","\n","        img = Image.open(image.path).convert('RGB')\n","        img_tensor = transforms.ToTensor()(img)\n","\n","        preds = model([img_tensor.to(device)])[0]\n","\n","        for i in range(len(preds[\"boxes\"])):\n","            x1, y1, x2, y2 = preds[\"boxes\"][i]\n","            label = int(preds[\"labels\"][i])\n","            score = float(preds[\"scores\"][i])\n","\n","            left = int(x1)\n","            top = int(y1)\n","            width = int(x2 - x1)\n","            height = int(y2 - y1)\n","\n","            if score >= det_threshold:\n","                detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hGPpF_F7QX98"},"source":["test_pred_json = os.path.join(base_folder, \"predictions.json\")\n","with open(test_pred_json, 'w') as f:\n","    json.dump(detections, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Abf5SW6790Zq"},"source":["Predictions `prediction.json` or the saved model `object_detection_rcnn.pt` can be found in `base_folder` defined above."]},{"cell_type":"code","metadata":{"id":"TksBH_P0tU1_"},"source":[""],"execution_count":null,"outputs":[]}]}